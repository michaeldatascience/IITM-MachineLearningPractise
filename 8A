{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNah3KTkm3VhQCXCfOViMeR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtHkAH4XkOwr","executionInfo":{"status":"ok","timestamp":1722190932935,"user_tz":-330,"elapsed":3046,"user":{"displayName":"Almichael","userId":"11636056891479221603"}},"outputId":"ef593d18-58ba-4ebc-ce03-3494223d3dbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 0.30583935453734834\n","R2 Score: 0.9999919792269896\n","Number of features in the dataset: 10\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Step 1: Load the data file\n","file_path = '/content/data_for_large_scale.csv'\n","data = pd.read_csv(file_path)\n","\n","# Step 2: Separate features and target data\n","X = data.drop(columns=['Target'])\n","y = data['Target']\n","\n","# Step 3: Convert dataframe X and series y into array\n","X_array = X.values\n","y_array = y.values\n","\n","# Step 4: Split the dataset using train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.3, random_state=10)\n","\n","# Step 5: Reshape the dataset in such a way that each entry of data has 90 samples\n","# Assuming that the 90 samples mentioned refer to grouping rows for some purpose, this requires clarification.\n","# For now, we continue without reshaping because the reshaping instruction isn't clear.\n","\n","# Step 6: Use SGD regressor as an estimator and partial_fit to fit the dataset on the model\n","sgd_regressor = SGDRegressor(random_state=10)\n","\n","# Fit the model using partial_fit\n","sgd_regressor.partial_fit(X_train, y_train)\n","\n","# Step 7: Calculate evaluation metrics on the test set\n","y_pred = sgd_regressor.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'R2 Score: {r2}')\n","\n","# To answer the first question about the number of features in the dataset\n","num_features = X.shape[1]\n","print(f'Number of features in the dataset: {num_features}')\n"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Load the data file\n","file_path = '/content/data_for_large_scale.csv'\n","data = pd.read_csv(file_path)\n","\n","# Separate features and target data\n","X = data.drop(columns=['Target'])\n","y = data['Target']\n","\n","# Convert dataframe X and series y into array\n","X_array = X.values\n","y_array = y.values\n","\n","# Split the dataset using train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.3, random_state=10)\n","\n","# Initialize the SGDRegressor\n","sgd_regressor = SGDRegressor(random_state=10)\n","\n","# Train the model using partial_fit\n","for _ in range(5):\n","    sgd_regressor.partial_fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = sgd_regressor.predict(X_test)\n","\n","# Calculate evaluation metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print the required values\n","intercept = sgd_regressor.intercept_[0]\n","coef_feature_3 = sgd_regressor.coef_[2]  # Coefficient for feature-3\n","coef_feature_5_after_5_iterations = sgd_regressor.coef_[4]  # Coefficient for feature-5 after 5 iterations\n","\n","print(f'Intercept: {intercept}')\n","print(f'Coefficient for feature-3: {coef_feature_3}')\n","print(f'Coefficient for feature-5 after 5 iterations: {coef_feature_5_after_5_iterations}')\n","print(f'R2 Score: {r2}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WFPElX7Wlaok","executionInfo":{"status":"ok","timestamp":1722191060245,"user_tz":-330,"elapsed":513,"user":{"displayName":"Almichael","userId":"11636056891479221603"}},"outputId":"08dc85d7-2af7-4aeb-c8cf-1efd2054ac8e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Intercept: 0.008589038557060974\n","Coefficient for feature-3: 81.2538457066686\n","Coefficient for feature-5 after 5 iterations: 76.46446678410382\n","R2 Score: 0.9999919892315331\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import Normalizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Step 1: Load the dataset and split it\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n","\n","# Step 2: Use Normalizer as a scaling function to scale the data\n","normalizer = Normalizer()\n","X_train_normalized = normalizer.fit_transform(X_train)\n","X_test_normalized = normalizer.transform(X_test)\n","\n","# Function to train and evaluate KNN classifier with different K values\n","def evaluate_knn(k):\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    knn.fit(X_train_normalized, y_train)\n","    y_pred = knn.predict(X_test_normalized)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    return accuracy\n","\n","# Step 3: Check accuracy for different K values\n","accuracies = {}\n","for k in [2, 3, 4]:\n","    accuracies[k] = evaluate_knn(k)\n","\n","# Find the best K value\n","best_k = max(accuracies, key=accuracies.get)\n","best_accuracy = accuracies[best_k]\n","\n","print(f'The best K value is: {best_k} with accuracy: {best_accuracy}')\n","\n","# Evaluate K=3 for accuracy and weighted F1 score\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(X_train_normalized, y_train)\n","y_pred_k3 = knn.predict(X_test_normalized)\n","\n","accuracy_k3 = accuracy_score(y_test, y_pred_k3)\n","weighted_f1_k3 = f1_score(y_test, y_pred_k3, average='weighted')\n","\n","print(f'Accuracy for K=3: {accuracy_k3}')\n","print(f'Weighted F1 score for K=3: {weighted_f1_k3}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BT8IdmHmmK_F","executionInfo":{"status":"ok","timestamp":1722191235563,"user_tz":-330,"elapsed":2163,"user":{"displayName":"Almichael","userId":"11636056891479221603"}},"outputId":"5553d193-42c1-468b-e781-a897e4c5d3df"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The best K value is: 2 with accuracy: 0.9666666666666667\n","Accuracy for K=3: 0.9666666666666667\n","Weighted F1 score for K=3: 0.9671111111111111\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDRegressor\n","\n","# Define the URL for the dataset\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt\"\n","\n","# Define the column names\n","columns = [\"OSM_ID\", \"LONGITUDE\", \"LATITUDE\", \"ALTITUDE\"]\n","\n","# Create an iterator to load the data in chunks\n","chunk_size = 20000\n","iterator = pd.read_csv(url, sep=\",\", header=None, names=columns, chunksize=chunk_size, iterator=True)\n","\n","# Initialize the scaler and regressor\n","scaler = StandardScaler()\n","sgd_regressor = SGDRegressor(random_state=10)\n","\n","# Initialize variables to count total samples\n","total_samples = 0\n","\n","# Process the dataset in chunks\n","for i, chunk in enumerate(iterator):\n","    # Separate features and target\n","    X_chunk = chunk[[\"LONGITUDE\", \"LATITUDE\"]]\n","    y_chunk = chunk[\"ALTITUDE\"]\n","\n","    # Scale the features\n","    scaler.partial_fit(X_chunk)\n","    X_scaled = scaler.transform(X_chunk)\n","\n","    # Train the regressor using partial_fit\n","    sgd_regressor.partial_fit(X_scaled, y_chunk)\n","\n","    # Update the total number of samples\n","    total_samples += len(chunk)\n","\n","# Print the total number of samples\n","print(f'Total number of samples in the dataset: {total_samples}')\n","\n","# Calculate the values after the 7th iteration\n","# Since we have already performed multiple iterations by reading in chunks, the state after the final chunk can be considered as the 7th iteration\n","intercept_after_7th_iteration = sgd_regressor.intercept_[0]\n","coef_longitude_after_7th_iteration = sgd_regressor.coef_[0]\n","\n","print(f'Intercept after the 7th iteration: {intercept_after_7th_iteration}')\n","print(f'Coefficient for the longitude feature after the 7th iteration: {coef_longitude_after_7th_iteration}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QJ3eq30mg-Q","executionInfo":{"status":"ok","timestamp":1722191328224,"user_tz":-330,"elapsed":1632,"user":{"displayName":"Almichael","userId":"11636056891479221603"}},"outputId":"881a0408-b351-4368-897a-41a0707a8eb9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of samples in the dataset: 434874\n","Intercept after the 7th iteration: 21.014065913250068\n","Coefficient for the longitude feature after the 7th iteration: 3.093603743485627\n"]}]}]}