{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jDll6OUg0RC367TtCtEGaKPfSh2QMCWy","timestamp":1721929067609}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Practice Questions MLP Boot Camp Day4"],"metadata":{"id":"4AEncNaeOYMx"}},{"cell_type":"markdown","source":["Download the data from: https://drive.google.com/drive/folders/1bseMSE_CMSvxHqH2O-utb-pqailyo3lQ?usp=sharing **(Download MLP Boot Camp dataset Day4)**\n","Upload it on your google drive(Upload it to a folder) and mount your google drive. Once it's done, click on `Files` option on the left sidebar of your colab notebook. You will be able to locate your Google Drive(My Drive) & locate the train.csv file. **Copy the path**."],"metadata":{"id":"U_zz7aTDOJXx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"sJGNRTA4QvUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"<Paste the path here>\")\n","df.head()"],"metadata":{"id":"nJLAPY3bQwLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the dataset.\n","- The last column is the target column.\n","- Last 30% rows of the dataset constitute test set and remaining rows form the training set.\n","- Do not shuffle the dataset while splitting\n","- You must have to use only training set to train all the estimator in questions below.\n","- First row of the file has column names/ids, and it has no index column."],"metadata":{"id":"mCLzGWwdIJ3o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YktATNxwH--d"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### Instructions (Q2-Q3)\n","Instantiate a perceptron classifier that with following parameters:\n","  * random_state = 1729\n","  * learning rate = 1\n","  * Train for appropriate number of iterations\n","  * Do not shuffle the dataset for each iteration.\n","  * Include the intercept (bias) term.\n","  * Use 10% of the data for validation fraction.\n","  * Do not apply regularization.\n","  * Set warm start to true.\n","\n","Hint: one iteration of training indicates going over each sample exactly once.\n","\n","Train the classifier on the training data."],"metadata":{"id":"HNvNiLJSIVBc"}},{"cell_type":"code","source":[],"metadata":{"id":"F53Agcn_IXZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (Q2) Train the perceptron classifier for 5 iterations. What is value of bias (intercept) after 5th iteration?"],"metadata":{"id":"dF7Em1w7Ihb-"}},{"cell_type":"code","source":[],"metadata":{"id":"-bLykQV_Ig_G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Q.3 In continuation of the previous question, compute precision on training data for positive class (i.e. class value 1), after 5 iterations.\n","\n","[Hint: Use estimator trained from the previous question]\n"],"metadata":{"id":"ST2qqb5xI36y"}},{"cell_type":"code","source":[],"metadata":{"id":"bUIUpdCiI3SL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q.4 Train (on training data only) logistic regression using `SGDClassifier`. Use the following parameters:\n","\n","1. Choose appropriate loss value to obtain logistic regression\n","2. penalty='l2',\n","3. eta0=0.001,\n","4. alpha=0,\n","5. learning_rate='constant'\n","6. random_state=1729.\n","7. warm_start = True\n","\n","Train the classifier for 5 iterations and note the value of the loss in each iteration. What will be the loss value after second iteration?\n","\n","**Note:** Set the remaining parameters, if any, accordingly to be able to get the loss value after second iteration. Also note that the classifier has to be trained for 5 iterations."],"metadata":{"id":"1niiPDVcI9xx"}},{"cell_type":"code","source":[],"metadata":{"id":"J8Tzt--oJGpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q.5 Use GridSearchCV with SGDClassifier. Following are the classifier's parameters:\n","\n","- `loss = 'log_loss'`\n","- `learning_rate = 'constant'`\n","- `random_state = 1729`\n","\n","Following are parameters to examine:\n","-  `alpha = [0.0001, 0.0005, 0.001, 0.005]`\n","- `eta0 = [0.01, 0.05, 0.1, 0.5]`\n","\n","What are the best values of alpha and eta0 ?"],"metadata":{"id":"JgnX4v00JT6M"}},{"cell_type":"code","source":[],"metadata":{"id":"glsI6jXOJX-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q.6 Create a new estimator using SGDClassifier that uses the best parameters obtained in Gridsearch earlier ((learning rate to be constant, random_state to be '1729' and use appropriate loss for logistic regression)) and set the weight of class 0 to be 0.1 and the weight of class 1 to be 2. How many samples of class 1 from the test set are correctly predicted by this estimator?"],"metadata":{"id":"poxvkAxgJYfa"}},{"cell_type":"code","source":[],"metadata":{"id":"bok8KsagJkj3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Q.7 Fit an SVM classifier with following parameters:\n","\n","- `kernel='rbf'`\n","- `decision_function_shape='ovr'`\n","- random_state=`1729`  \n","- `C=1`\n","\n","Train the model on training data, and print the confusion matrix on test data."],"metadata":{"id":"DCBhW-OKJoCL"}},{"cell_type":"code","source":[],"metadata":{"id":"GZ4L3WQ6JzV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Instructions for Q8-10\n","Train a Decision Tree Classifier with the following properties:\n","\n","- criterion = 'entropy',\n","- splitter = 'random',\n","- min_samples_split = 4,\n","- min_impurity_decrease = 0.0001,\n","- random_state = 1729\n"],"metadata":{"id":"GQr5JstPJzrz"}},{"cell_type":"markdown","source":["###Q.8 What is the resultant depth of the tree?"],"metadata":{"id":"AYyX5H4bKBpT"}},{"cell_type":"code","source":[],"metadata":{"id":"qnMHtlxaKH-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q.9 How many nodes are there in the tree?"],"metadata":{"id":"HNl3WkxbKIX_"}},{"cell_type":"code","source":[],"metadata":{"id":"akAZNLi0KQzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Q.10 What is the value of entropy at the left and right child of root and what is the difference of entropy between root node and its child nodes?"],"metadata":{"id":"VPg0g74sKReq"}},{"cell_type":"code","source":[],"metadata":{"id":"vwYb_CqGKwTy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Q. 11 Out of DecisionTreeClassifier, KNeighborsClassifier and LogisticRegression, which one performs the best when used as base estimator in BaggingClassifier on the test data in terms of accuracy score when 20 base estimators are used ?\n","\n","(Use random state 1729 for BaggingClassifier, DecisionTreeClassifier and LogisticRegression)\n","\n","The metric for best performance will be the lowest 'absolute' difference in the train and test score.\n","\n","A. DecisionTreeClassifier\n","\n","B. KNeighborsClassifier\n","\n","C. LogisticRegression"],"metadata":{"id":"0rlSj6alKw1X"}},{"cell_type":"code","source":[],"metadata":{"id":"d4HzBkvqK8eG"},"execution_count":null,"outputs":[]}]}